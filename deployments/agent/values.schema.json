{"$schema":"http://json-schema.org/draft-07/schema#","$ref":"#/$defs/helm-values","$defs":{"helm-values":{"type":"object","properties":{"affinity":{"$ref":"#/$defs/helm-values.affinity"},"componentStatusSender":{"$ref":"#/$defs/helm-values.componentStatusSender"},"debugLogging":{"$ref":"#/$defs/helm-values.debugLogging"},"enable":{"$ref":"#/$defs/helm-values.enable"},"envoyClusterId":{"$ref":"#/$defs/helm-values.envoyClusterId"},"fullnameOverride":{"$ref":"#/$defs/helm-values.fullnameOverride"},"global":{"$ref":"#/$defs/helm-values.global"},"httpPort":{"$ref":"#/$defs/helm-values.httpPort"},"image":{"$ref":"#/$defs/helm-values.image"},"nameOverride":{"$ref":"#/$defs/helm-values.nameOverride"},"nodeSelector":{"$ref":"#/$defs/helm-values.nodeSelector"},"podAnnotations":{"$ref":"#/$defs/helm-values.podAnnotations"},"proxy":{"$ref":"#/$defs/helm-values.proxy"},"readinessProbe":{"$ref":"#/$defs/helm-values.readinessProbe"},"replicaCount":{"$ref":"#/$defs/helm-values.replicaCount"},"resources":{"$ref":"#/$defs/helm-values.resources"},"serviceAccount":{"$ref":"#/$defs/helm-values.serviceAccount"},"sessionManagerServerWorkerServiceAddr":{"$ref":"#/$defs/helm-values.sessionManagerServerWorkerServiceAddr"},"tolerations":{"$ref":"#/$defs/helm-values.tolerations"},"version":{"$ref":"#/$defs/helm-values.version"},"volumeMounts":{"$ref":"#/$defs/helm-values.volumeMounts"},"volumes":{"$ref":"#/$defs/helm-values.volumes"}},"additionalProperties":false},"helm-values.affinity":{"description":"A Kubernetes Affinity, if required.\nFor more information, see [Assigning Pods to Nodes](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node).\n\nFor example:\naffinity:\n  nodeAffinity:\n   requiredDuringSchedulingIgnoredDuringExecution:\n     nodeSelectorTerms:\n     - matchExpressions:\n       - key: foo.bar.com/role\n         operator: In\n         values:\n         - master","type":"object"},"helm-values.componentStatusSender":{"type":"object","properties":{"clusterManagerServerWorkerServiceAddr":{"$ref":"#/$defs/helm-values.componentStatusSender.clusterManagerServerWorkerServiceAddr"},"enable":{"$ref":"#/$defs/helm-values.componentStatusSender.enable"},"initialDelay":{"$ref":"#/$defs/helm-values.componentStatusSender.initialDelay"},"interval":{"$ref":"#/$defs/helm-values.componentStatusSender.interval"},"name":{"$ref":"#/$defs/helm-values.componentStatusSender.name"}},"additionalProperties":false},"helm-values.componentStatusSender.clusterManagerServerWorkerServiceAddr":{"description":"The address of the cluster-manager-server to call worker services.","type":"string","default":"cluster-manager-server-worker-service-grpc:8082"},"helm-values.componentStatusSender.enable":{"description":"The flag to enable the component status sender.","type":"boolean","default":true},"helm-values.componentStatusSender.initialDelay":{"description":"initialDelay is the time to wait before starting the sender.","type":"string","default":"1m"},"helm-values.componentStatusSender.interval":{"description":"The interval time to send the component status.","type":"string","default":"15m"},"helm-values.componentStatusSender.name":{"description":"The name of the component.","type":"string","default":"session-manager-agent"},"helm-values.debugLogging":{"description":"If enabled, show the debugging logs of the session-manager-agent.","type":"boolean","default":true},"helm-values.enable":{"description":"This field can be used as a condition when using it as a dependency. This definition is only here as a placeholder such that it is included in the json schema.","type":"boolean"},"helm-values.envoyClusterId":{"description":"The ID of this cluster.","type":"string","default":"local-cluster-id"},"helm-values.fullnameOverride":{"description":"Override the \"session-manager-agent.fullname\" value. This value is used as part of most of the names of the resources created by this Helm chart.","type":"string"},"helm-values.global":{"description":"Global values shared across all (sub)charts","type":"object","properties":{"worker":{"$ref":"#/$defs/helm-values.global.worker"}}},"helm-values.global.worker":{"type":"object","properties":{"controlPlaneAddr":{"$ref":"#/$defs/helm-values.global.worker.controlPlaneAddr"},"registrationKeySecret":{"$ref":"#/$defs/helm-values.global.worker.registrationKeySecret"},"tls":{"$ref":"#/$defs/helm-values.global.worker.tls"}}},"helm-values.global.worker.controlPlaneAddr":{"description":"If specified, use this address for accessing the control-plane. This is necessary when installing LLMariner in a multi-cluster mode. For more information, see [Install across Multiple Clusters](https://llmariner.ai/docs/setup/install/multi_cluster_production/).","type":"string","default":""},"helm-values.global.worker.registrationKeySecret":{"type":"object","properties":{"key":{"$ref":"#/$defs/helm-values.global.worker.registrationKeySecret.key"},"name":{"$ref":"#/$defs/helm-values.global.worker.registrationKeySecret.name"}}},"helm-values.global.worker.registrationKeySecret.key":{"description":"The key name with a registration key set.","type":"string","default":"key"},"helm-values.global.worker.registrationKeySecret.name":{"description":"The secret name. `default-cluster-registration-key` is available when the control-plane and worker-plane are in the same cluster. This Secret is generated by cluster-manager-server as default. For more information, see [Install across Multiple Clusters](https://llmariner.ai/docs/setup/install/multi_cluster_production/).","type":"string","default":"default-cluster-registration-key"},"helm-values.global.worker.tls":{"type":"object","properties":{"enable":{"$ref":"#/$defs/helm-values.global.worker.tls.enable"}}},"helm-values.global.worker.tls.enable":{"description":"The flag to enable TLS access to the control-plane.","type":"boolean","default":false},"helm-values.httpPort":{"description":"The HTTP port number.","type":"number","default":8080},"helm-values.image":{"type":"object","properties":{"pullPolicy":{"$ref":"#/$defs/helm-values.image.pullPolicy"},"repository":{"$ref":"#/$defs/helm-values.image.repository"}},"additionalProperties":false},"helm-values.image.pullPolicy":{"description":"Kubernetes imagePullPolicy on Deployment.","type":"string","default":"IfNotPresent"},"helm-values.image.repository":{"description":"The container image name.","type":"string","default":"public.ecr.aws/cloudnatix/llmariner/session-manager-agent"},"helm-values.nameOverride":{"description":"Override the \"session-manager-agent.name\" value, which is used to annotate some of the resources that are created by this Chart (using \"app.kubernetes.io/name\").","type":"string"},"helm-values.nodeSelector":{"description":"The nodeSelector on Pods tells Kubernetes to schedule Pods on the nodes with matching labels. For more information, see [Assigning Pods to Nodes](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/).","type":"object"},"helm-values.podAnnotations":{"description":"Optional additional annotations to add to the Deployment Pods.","type":"object"},"helm-values.proxy":{"type":"object","properties":{"baseUrl":{"$ref":"#/$defs/helm-values.proxy.baseUrl"},"http":{"$ref":"#/$defs/helm-values.proxy.http"},"upgrade":{"$ref":"#/$defs/helm-values.proxy.upgrade"}},"additionalProperties":false},"helm-values.proxy.baseUrl":{"description":"Deprecated. Use sessionManagerServerWorkerServiceAddr instead.","type":"string","default":""},"helm-values.proxy.http":{"type":"object","properties":{"poolSize":{"$ref":"#/$defs/helm-values.proxy.http.poolSize"}},"additionalProperties":false},"helm-values.proxy.http.poolSize":{"description":"pool size of the HTTP tunnel. Currently server supports only one channel per agent.","type":"number","default":1},"helm-values.proxy.upgrade":{"type":"object","properties":{"poolSize":{"$ref":"#/$defs/helm-values.proxy.upgrade.poolSize"}},"additionalProperties":false},"helm-values.proxy.upgrade.poolSize":{"description":"pool size of the HTTP upgrade tunnel","type":"number","default":10},"helm-values.readinessProbe":{"type":"object","properties":{"failureThreshold":{"$ref":"#/$defs/helm-values.readinessProbe.failureThreshold"},"initialDelaySeconds":{"$ref":"#/$defs/helm-values.readinessProbe.initialDelaySeconds"},"periodSeconds":{"$ref":"#/$defs/helm-values.readinessProbe.periodSeconds"},"successThreshold":{"$ref":"#/$defs/helm-values.readinessProbe.successThreshold"},"timeoutSeconds":{"$ref":"#/$defs/helm-values.readinessProbe.timeoutSeconds"}},"additionalProperties":false},"helm-values.readinessProbe.failureThreshold":{"description":"After a probe fails `failureThreshold` times in a row, Kubernetes considers that the overall check has failed: the container is not ready/healthy/live.","type":"number","default":3},"helm-values.readinessProbe.initialDelaySeconds":{"description":"Number of seconds after the container has started before startup, liveness or readiness probes are initiated.","type":"number","default":5},"helm-values.readinessProbe.periodSeconds":{"description":"How often (in seconds) to perform the probe.","type":"number","default":60},"helm-values.readinessProbe.successThreshold":{"description":"Minimum consecutive successes for the probe to be considered successful after having failed.","type":"number","default":1},"helm-values.readinessProbe.timeoutSeconds":{"description":"Number of seconds after which the probe times out.","type":"number","default":5},"helm-values.replicaCount":{"description":"The number of replicas for the session-manager-agent Deployment.","type":"number","default":1},"helm-values.resources":{"description":"Resources to provide to the session-manager-agent pod. For more information, see [Resource Management for Pods and Containers](https://kubernetes.io/docs/concepts/configuration/manage-resources-Containers/).\n\nFor example:\nrequests:\n  cpu: 10m\n  memory: 32Mi","type":"object","default":{"limits":{"cpu":"250m"},"requests":{"cpu":"250m","memory":"500Mi"}}},"helm-values.serviceAccount":{"type":"object","properties":{"create":{"$ref":"#/$defs/helm-values.serviceAccount.create"},"name":{"$ref":"#/$defs/helm-values.serviceAccount.name"}},"additionalProperties":false},"helm-values.serviceAccount.create":{"description":"Specifies whether a service account should be created.","type":"boolean","default":true},"helm-values.serviceAccount.name":{"description":"The name of the service account to use. If not set and create is true, a name is generated using the fullname template.","type":"string"},"helm-values.sessionManagerServerWorkerServiceAddr":{"description":"This default value works if session-manager-server runs in the same namespace.","type":"string","default":"session-manager-server-worker-service-http:8082"},"helm-values.tolerations":{"description":"A list of Kubernetes Tolerations, if required.\nFor more information, see [Taints and Tolerations](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/).\n\nFor example:\ntolerations:\n- key: foo.bar.com/role\n  operator: Equal\n  value: master\n  effect: NoSchedule","type":"array","items":{}},"helm-values.version":{"description":"Override the container image tag to deploy by setting this variable. If no value is set, the chart's appVersion will be used.","type":"string"},"helm-values.volumeMounts":{"description":"Additional volume mounts to add to the session-manager-server container. For more information, see [Volumes](https://kubernetes.io/docs/concepts/storage/volumes/).","type":"array","items":{}},"helm-values.volumes":{"description":"Additional volumes to add to the session-manager-server pod. For more information, see [Volumes](https://kubernetes.io/docs/concepts/storage/volumes/).","type":"array","items":{}}}}
